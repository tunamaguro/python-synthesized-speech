{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "smoking-somalia",
   "metadata": {},
   "source": [
    "# 第7章 WaveNet: 深層学習に基づく音声波形の生成モデル\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/r9y9/ttslearn/blob/master/notebooks/ch07_WaveNet.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-radical",
   "metadata": {},
   "source": [
    "## 準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-joseph",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -VV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-subsection",
   "metadata": {},
   "source": [
    "### ttslearn のインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-combining",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "try:\n",
    "    import ttslearn\n",
    "except ImportError:\n",
    "    !pip install ttslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-serve",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttslearn\n",
    "ttslearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-flexibility",
   "metadata": {},
   "source": [
    "### パッケージのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%load_ext tensorboard\n",
    "%autoreload\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "import tensorboard as tb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数値演算\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "# 音声波形の読み込み\n",
    "from scipy.io import wavfile\n",
    "# 音声分析、可視化\n",
    "import librosa\n",
    "import librosa.display\n",
    "# Pythonで学ぶ音声合成\n",
    "import ttslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# シードの固定\n",
    "from ttslearn.util import init_seed\n",
    "init_seed(773)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-arabic",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-confusion",
   "metadata": {},
   "source": [
    "### 描画周りの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.notebook import get_cmap, init_plot_style, savefig\n",
    "cmap = get_cmap()\n",
    "init_plot_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-symbol",
   "metadata": {},
   "source": [
    "## 7.3 WaveNetにおける音声波形の扱い"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-sender",
   "metadata": {},
   "source": [
    "### $\\mu$-law アルゴリズム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mulaw(x, mu=255):\n",
    "    return np.sign(x) * np.log1p(mu * np.abs(x)) / np.log1p(mu)\n",
    "\n",
    "def quantize(y, mu=255, offset=1):\n",
    "    # [-1, 1] -> [0, 2] -> [0, 1] -> [0, mu]\n",
    "    return ((y + offset) / 2 * mu).astype(np.int64)    \n",
    "\n",
    "def mulaw_quantize(x, mu=255):\n",
    "    return quantize(mulaw(x, mu), mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-cinema",
   "metadata": {},
   "source": [
    "#### $\\mu$-law アルゴリズム適用前"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "x = (x / 32768.0).astype(np.float32)\n",
    "\n",
    "mu = 2**8-1 # 8-bit\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(6,4))\n",
    "ax[0].set_title(\"Waveform\")\n",
    "ax[1].set_title(\"Histrogram\")\n",
    "\n",
    "ax[0].set_ylim(-0.9, 0.9)\n",
    "librosa.display.waveshow(x, ax=ax[0], sr=16000)\n",
    "\n",
    "ax[1].set_xlim(-0.9, 0.9)\n",
    "ax[1].hist(x, bins=mu)\n",
    "\n",
    "ax[0].set_xlabel(\"Time [sec]\")\n",
    "ax[0].set_ylabel(\"Amplitude\")\n",
    "ax[1].set_xlabel(\"Amplitude\")\n",
    "ax[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# 図7-6 (a)\n",
    "savefig(\"./fig/wavenet_mulaw_a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-supplier",
   "metadata": {},
   "source": [
    "#### $\\mu$-law アルゴリズム適用後"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(6,4))\n",
    "ax[0].set_title(\"Waveform\")\n",
    "ax[1].set_title(\"Histrogram\")\n",
    "\n",
    "ax[0].set_ylim(-0.9, 0.9)\n",
    "librosa.display.waveshow(mulaw(x), ax=ax[0], sr=16000)\n",
    "\n",
    "ax[1].set_xlim(-0.9, 0.9)\n",
    "ax[1].hist(mulaw(x), bins=mu)\n",
    "\n",
    "ax[0].set_xlabel(\"Time [sec]\")\n",
    "ax[0].set_ylabel(\"Amplitude\")\n",
    "ax[1].set_xlabel(\"Amplitude\")\n",
    "ax[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# 図7-6 (b)\n",
    "savefig(\"./fig/wavenet_mulaw_b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-junior",
   "metadata": {},
   "source": [
    "### $\\mu$-law アルゴリズムによる逆変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_mulaw(y, mu=255):\n",
    "    return np.sign(y) * (1.0 / mu) * ((1.0 + mu)**np.abs(y) - 1.0)\n",
    "\n",
    "def inv_quantize(y, mu):\n",
    "    # [0, mu] -> [-1, 1]\n",
    "    return 2 * y.astype(np.float32) / mu - 1\n",
    "\n",
    "def inv_mulaw_quantize(y, mu=255):\n",
    "    return inv_mulaw(inv_quantize(y, mu), mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-diesel",
   "metadata": {},
   "source": [
    "#### $\\mu$-law なし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-perspective",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "x = (x / 32768.0).astype(np.float32)  \n",
    "x = librosa.resample(x, sr, 16000)\n",
    "sr = 16000\n",
    "\n",
    "bits = [8, 4]\n",
    "\n",
    "fig, ax = plt.subplots(len(bits)+1, 1, figsize=(6,2*(len(bits)+1)), sharey=True)\n",
    "ax[0].set_title(\"Input waveform\")\n",
    "librosa.display.waveshow(x, sr, x_axis=\"time\", ax=ax[0])\n",
    "IPython.display.display(Audio(x, rate=sr))\n",
    "\n",
    "for idx, bit in enumerate(bits):\n",
    "    mu = 2**bit - 1\n",
    "    x_hat = inv_quantize(quantize(x, mu), mu)\n",
    "    librosa.display.waveshow(x_hat, sr, x_axis=\"time\", ax=ax[idx+1])\n",
    "    ax[idx+1].set_title(f\"{bit}-bit waveform\")\n",
    "    IPython.display.display(Audio(x_hat, rate=sr))\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    a.set_ylabel(\"Amplitude\")\n",
    "    a.set_xticks(np.arange(0, 3.5, 0.5))\n",
    "    a.set_ylim(-0.5, 0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "# 図7-7 (a)\n",
    "savefig(\"./fig/wavenet_inv_mulaw_waveform_a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-maximum",
   "metadata": {},
   "source": [
    "#### $\\mu$-law あり"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "x = (x / 32768.0).astype(np.float32)  \n",
    "x = librosa.resample(x, sr, 16000)\n",
    "sr = 16000\n",
    "\n",
    "bits = [8, 4]\n",
    "\n",
    "fig, ax = plt.subplots(len(bits)+1, 1, figsize=(6,2*(len(bits)+1)), sharey=True)\n",
    "ax[0].set_title(\"Input waveform\")\n",
    "librosa.display.waveshow(x, sr, x_axis=\"time\", ax=ax[0])\n",
    "IPython.display.display(Audio(x, rate=sr))\n",
    "\n",
    "for idx, bit in enumerate(bits):\n",
    "    mu = 2**bit - 1\n",
    "    x_hat = inv_mulaw_quantize(mulaw_quantize(x, mu), mu)\n",
    "    librosa.display.waveshow(x_hat, sr, x_axis=\"time\", ax=ax[idx+1])\n",
    "    ax[idx+1].set_title(f\"{bit}-bit waveform\")\n",
    "    IPython.display.display(Audio(x_hat, rate=sr))\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    a.set_ylabel(\"Amplitude\")\n",
    "    a.set_xticks(np.arange(0, 3.5, 0.5))\n",
    "    a.set_ylim(-0.5, 0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "# 図7-7 (b)\n",
    "savefig(\"./fig/wavenet_inv_mulaw_waveform_b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-green",
   "metadata": {},
   "source": [
    "## 7.4 因果的な膨張畳み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-burst",
   "metadata": {},
   "source": [
    "### 1次元の畳み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _toy_1d_input():\n",
    "    # (B, C, T) where B and C = 1\n",
    "    return torch.tensor([1,2,3,0,1,2,4],dtype=torch.float).view(1,1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-creek",
   "metadata": {},
   "source": [
    "#### パディングを行わない場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv1d(1,1,3,bias=False, padding=0)\n",
    "conv.weight.data[0,0,:] = torch.tensor([1,2,4],dtype=torch.float)\n",
    "\n",
    "x = _toy_1d_input()\n",
    "with torch.no_grad():\n",
    "    y= conv(x)\n",
    "print(\"入力:\", x.long().view(-1).tolist())\n",
    "print(\"出力:\", y.long().view(-1).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-secret",
   "metadata": {},
   "source": [
    "#### パディングを行う場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv1d(1,1,3,bias=False, padding=1)\n",
    "conv.weight.data[0,0,:] = torch.tensor([1,2,4],dtype=torch.float)\n",
    "\n",
    "x = _toy_1d_input()\n",
    "with torch.no_grad():\n",
    "    y= conv(x)\n",
    "print(\"入力:\", x.long().view(-1).tolist())\n",
    "print(\"出力:\", y.long().view(-1).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-admission",
   "metadata": {},
   "source": [
    "#### 2層の1次元畳み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv1d(1,1,3,bias=False, padding=1)\n",
    "conv.weight.data[0,0,:] = torch.tensor([1,2,4],dtype=torch.float)\n",
    "\n",
    "x = _toy_1d_input()\n",
    "with torch.no_grad():\n",
    "    y= conv(conv(x))\n",
    "print(\"入力:\", x.long().view(-1).tolist())\n",
    "print(\"出力:\", y.long().view(-1).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-disaster",
   "metadata": {},
   "source": [
    "### 因果的な畳み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-bandwidth",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n",
    "        super().__init__()\n",
    "        self.padding = (kernel_size - 1)\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=self.padding, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1 次元畳み込み\n",
    "        y = self.conv(x)\n",
    "        # 因果性を担保するために、順方向にシフトする\n",
    "        if self.padding > 0:\n",
    "            y = y[:, :, :-self.padding]\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = CausalConv1d(1,1,3,bias=False)\n",
    "# テスト用に、畳み込みカーネルを手動で設定\n",
    "conv.conv.weight.data[0,0,:] = torch.tensor([1,2,4],dtype=torch.float)\n",
    "\n",
    "x = _toy_1d_input()\n",
    "y= conv(x)\n",
    "print(\"入力:\", x.long().view(-1).tolist())\n",
    "print(\"出力:\", y.long().view(-1).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-agenda",
   "metadata": {},
   "source": [
    "### 1次元膨張畳み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DilatedCausalConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1, **kwargs):\n",
    "        super().__init__()\n",
    "        # パディングの幅を計算する際に、 dilation factor を考慮する必要があることに注意        \n",
    "        self.padding = (kernel_size - 1) * dilation\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=self.padding, dilation=dilation, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1 次元畳み込み        \n",
    "        y = self.conv(x)\n",
    "        # 因果性を担保するために、順方向にシフトする\n",
    "        if self.padding > 0:\n",
    "            y = y[:, :, :-self.padding]\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-trigger",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = DilatedCausalConv1d(1,1,3,dilation=2, bias=False)\n",
    "# テスト用に、畳み込みカーネルを手動で設定\n",
    "conv.conv.weight.data[0,0,:] = torch.tensor([1,2,4],dtype=torch.float)\n",
    "\n",
    "x = _toy_1d_input()\n",
    "y= conv(x)\n",
    "print(\"入力:\", x.long().view(-1).tolist())\n",
    "print(\"出力:\", y.long().view(-1).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-locking",
   "metadata": {},
   "source": [
    "## 7.5 ゲート付き活性化関数を用いた一次元畳み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-riding",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedDilatedCausalConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1):\n",
    "        super().__init__()\n",
    "        self.padding = (kernel_size - 1) * dilation\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels*2, kernel_size, padding=self.padding, dilation=dilation)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 1 次元畳み込み                \n",
    "        y = self.conv(x)\n",
    "        \n",
    "        # 因果性を担保するために、順方向にシフトする\n",
    "        if self.padding > 0:\n",
    "            y = y[:, :, :-self.padding]\n",
    "\n",
    "        # チャネル方向に分割\n",
    "        a, b = y.split(y.size(1) // 2, dim=1)\n",
    "        \n",
    "        # ゲート付き活性化関数の適用\n",
    "        y = torch.tanh(a) * torch.sigmoid(b)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-guarantee",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = GatedDilatedCausalConv1d(128, 16, 3, dilation=2)\n",
    "x = torch.ones(32, 128, 100)\n",
    "print(\"入力のサイズ：\", tuple(x.shape))\n",
    "print(\"出力のサイズ：\", tuple(conv(x).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-beatles",
   "metadata": {},
   "source": [
    "## 7.6 条件付け特徴量のアップサンプリング"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-cosmetic",
   "metadata": {},
   "source": [
    "### 繰り返しに基づくアップサンプリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 3],[1, 2, 3],[1,2,3]]).view(1,3,-1).float()\n",
    "y = nn.Upsample(scale_factor=3, mode=\"nearest\")(x)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepeatUpsampling(nn.Module):\n",
    "    def __init__(self, upsample_scales):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=np.prod(upsample_scales), mode=\"nearest\")\n",
    "\n",
    "    def forward(self, c):\n",
    "        return self.upsample(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-helicopter",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.ones(32, 80, 10)\n",
    "# 例として、100倍にアップサンプリング\n",
    "c_up = RepeatUpsampling([100])(c)\n",
    "\n",
    "print(\"入力のサイズ：\", tuple(c.shape))\n",
    "print(\"出力サイズ：\", tuple(c_up.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-adoption",
   "metadata": {},
   "source": [
    "### 最近傍補間と畳み込みの併用に基づくアップサンプリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class UpsampleNetwork(nn.Module):\n",
    "    def __init__(self, upsample_scales):\n",
    "        super().__init__()\n",
    "        self.upsample_scales = upsample_scales\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        for scale in upsample_scales:\n",
    "            kernel_size = (1, scale * 2 + 1)\n",
    "            conv = nn.Conv2d(\n",
    "                1, 1, kernel_size=kernel_size, padding=(0, scale), bias=False\n",
    "            )\n",
    "            conv.weight.data.fill_(1.0 / np.prod(kernel_size))\n",
    "            self.conv_layers.append(conv)\n",
    "\n",
    "    def forward(self, c):\n",
    "        # (B, 1, C, T)\n",
    "        c = c.unsqueeze(1)\n",
    "        # 最近傍補完と畳み込みの繰り返し\n",
    "        for idx, scale in enumerate(self.upsample_scales):\n",
    "            # 時間方向にのみアップサンプリング\n",
    "            # (B, 1, C, T) -> (B, 1, C, T*scale)\n",
    "            c = F.interpolate(c, scale_factor=(1, scale), mode=\"nearest\")\n",
    "            c = self.conv_layers[idx](c)\n",
    "        # B x C x T\n",
    "        return c.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.ones(32, 80, 10)\n",
    "c_up = UpsampleNetwork([10, 8])(c)\n",
    "\n",
    "print(\"入力のサイズ：\", tuple(c.shape))\n",
    "print(\"出力サイズ：\", tuple(c_up.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-alarm",
   "metadata": {},
   "source": [
    "#### 実データ (mel-spectrogram) のアップサンプリング (bonus)\n",
    "\n",
    "書籍では解説しませんでしたが、二次元畳み込みの重みを適切に初期化することで、畳み込みの前後でスケールが保持されることを示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-spiritual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期化の影響を確認するため、畳み込みのパラメータを乱数で初期化\n",
    "class RandomInitUpsampleNetwork(UpsampleNetwork):\n",
    "    def __init__(self, upsample_scales):\n",
    "        super().__init__(upsample_scales)\n",
    "        for conv in self.conv_layers:\n",
    "            nn.init.normal_(conv.weight.data, 0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-defensive",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.dsp import logmelspectrogram\n",
    "\n",
    "_sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "x = (x / 32768.0).astype(np.float32)\n",
    "sr = 16000\n",
    "x = librosa.resample(x, _sr, sr)\n",
    "hop_length = int(0.0125 * sr)\n",
    "sp = logmelspectrogram(x, sr, hop_length=hop_length)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "mesh = librosa.display.specshow(sp.T, sr=sr, hop_length=hop_length, cmap=cmap, x_axis=\"time\", y_axis=\"frames\")\n",
    "fig.colorbar(mesh, ax=ax)\n",
    "ax.set_xlabel(\"Time [sec]\")\n",
    "ax.set_ylabel(\"Frequency [Hz]\")\n",
    "plt.tight_layout()\n",
    "\n",
    "Audio(x, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample_net = UpsampleNetwork([10, 8])\n",
    "upsample_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsp = torch.from_numpy(sp.T).view(1, 80, -1)\n",
    "\n",
    "# 畳み込みのカーネルを適切に初期化した場合\n",
    "tsp_up = upsample_net(tsp)\n",
    "\n",
    "# ランダムに初期化した場合\n",
    "torch.manual_seed(0)\n",
    "upsample_net_rand_init = RandomInitUpsampleNetwork([10, 8])\n",
    "\n",
    "tsp_up_rand_init = upsample_net_rand_init(tsp)\n",
    "\n",
    "A = tsp.squeeze(0).numpy()\n",
    "B = tsp_up_rand_init.squeeze(0).detach().numpy()\n",
    "C = tsp_up.squeeze(0).detach().numpy()\n",
    "\n",
    "s, e = 100, 120\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10,5))\n",
    "ax[0].set_title(\"Mel-spectrogram\")\n",
    "ax[1].set_title(\"Upsample (random init)\")\n",
    "ax[2].set_title(\"Upsample (proper init)\")\n",
    "\n",
    "ax[0].set_xlim(s, e)\n",
    "ax[0].imshow(A, aspect=\"auto\", interpolation=\"nearest\", origin=\"lower\", cmap=cmap)\n",
    "fig.colorbar(ax[0].pcolormesh(A, cmap=cmap, rasterized=True), ax=ax[0])\n",
    "\n",
    "ax[1].set_xlim(s*80, e*80)\n",
    "ax[1].imshow(B, aspect=\"auto\", interpolation=\"nearest\", origin=\"lower\", cmap=cmap)\n",
    "fig.colorbar(ax[1].pcolormesh(B, cmap=cmap, rasterized=True), ax=ax[1])\n",
    "\n",
    "ax[2].set_xlim(s*80, e*80)\n",
    "ax[2].imshow(C, aspect=\"auto\", interpolation=\"nearest\", origin=\"lower\", cmap=cmap)\n",
    "fig.colorbar(ax[2].pcolormesh(C, cmap=cmap, rasterized=True), ax=ax[2])\n",
    "\n",
    "for a in ax:\n",
    "    # あとでラベルを付け直すので、ここでは消しておく\n",
    "    a.set_ylabel(\"\")\n",
    "\n",
    "ax[0].set_ylabel(\"Mel filter channel\")\n",
    "ax[0].set_xlabel(\"Time [frame]\")\n",
    "for a in ax[1:]:\n",
    "    a.set_xlabel(\"Time [sample]\")\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-business",
   "metadata": {},
   "source": [
    "### 近傍の条件付け特徴量を考慮するアップサンプリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-airline",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvInUpsampleNetwork(nn.Module):\n",
    "    def __init__(self, upsample_scales, cin_channels, aux_context_window):\n",
    "        super(ConvInUpsampleNetwork, self).__init__()\n",
    "        # 条件付き特徴量の時間方向の近傍情報を、1 次元畳み込みによって考慮する\n",
    "        kernel_size = 2 * aux_context_window + 1\n",
    "        self.conv_in = nn.Conv1d(cin_channels, cin_channels, kernel_size, bias=False)\n",
    "        # アップサンプリング\n",
    "        self.upsample = UpsampleNetwork(upsample_scales)\n",
    "\n",
    "    def forward(self, c):\n",
    "        c_up = self.upsample(self.conv_in(c))\n",
    "        return c_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.ones(32, 80, 10)\n",
    "\n",
    "c_up = ConvInUpsampleNetwork([10, 8], 80, 2)(c)\n",
    "print(\"入力のサイズ：\", tuple(c.shape))\n",
    "print(\"出力サイズ：\", tuple(c_up.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-wisdom",
   "metadata": {},
   "source": [
    "## 7.7 WaveNetの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-password",
   "metadata": {},
   "source": [
    "### 1 x 1 畳み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-despite",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv1d1x1(in_channels, out_channels, bias=True):\n",
    "    return nn.Conv1d(\n",
    "        in_channels, out_channels, kernel_size=1, padding=0, dilation=1, bias=bias\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-domain",
   "metadata": {},
   "source": [
    "\n",
    "### 畳み込みブロック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-electricity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResSkipBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        residual_channels,  # 残差結合のチャネル数\n",
    "        gate_channels,  # ゲートのチャネル数\n",
    "        kernel_size,  # カーネルサイズ\n",
    "        skip_out_channels,  # スキップ結合のチャネル数\n",
    "        dilation=1,  # dilation factor\n",
    "        cin_channels=80,  # 条件付特徴量のチャネル数\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.padding = (kernel_size - 1) * dilation\n",
    "\n",
    "        # 1 次元膨張畳み込み (dilation == 1 のときは、通常の 1 次元畳み込み)\n",
    "        self.conv = nn.Conv1d(\n",
    "            residual_channels,\n",
    "            gate_channels,\n",
    "            kernel_size,\n",
    "            padding=self.padding,\n",
    "            dilation=dilation,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # local conditioning 用の 1x1 convolution\n",
    "        self.conv1x1c = Conv1d1x1(cin_channels, gate_channels, bias=False)\n",
    "\n",
    "        # ゲート付き活性化関数のために、1 次元畳み込みの出力は 2 分割されることに注意\n",
    "        gate_out_channels = gate_channels // 2\n",
    "        self.conv1x1_out = Conv1d1x1(gate_out_channels, residual_channels)\n",
    "        self.conv1x1_skip = Conv1d1x1(gate_out_channels, skip_out_channels)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        # 残差接続用に入力を保持\n",
    "        residual = x\n",
    "\n",
    "        # 1 次元畳み込み\n",
    "        splitdim = 1  # (B, C, T)\n",
    "        x = self.conv(x)\n",
    "        # 因果性を保証するために、出力をシフトする\n",
    "        x = x[:, :, : -self.padding]\n",
    "\n",
    "        # チャネル方向で出力を分割\n",
    "        a, b = x.split(x.size(1) // 2, dim=1)\n",
    "\n",
    "        # local conditioning\n",
    "        c = self.conv1x1c(c)\n",
    "        ca, cb = c.split(c.size(1) // 2, dim=1)\n",
    "        a, b = a + ca, b + cb\n",
    "\n",
    "        # ゲート付き活性化関数\n",
    "        x = torch.tanh(a) * torch.sigmoid(b)\n",
    "\n",
    "        # スキップ接続用の出力を計算\n",
    "        s = self.conv1x1_skip(x)\n",
    "\n",
    "        # 残差接続の要素和を行う前に、次元数を合わせる\n",
    "        x = self.conv1x1_out(x)\n",
    "\n",
    "        x = x + residual\n",
    "\n",
    "        return x, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 3\n",
    "conv = ResSkipBlock(128,16,kernel_size, 64, dilation=4)\n",
    "x = torch.ones(32, 128, 100)\n",
    "c = torch.ones(32, 80, 100)\n",
    "out, skip = conv(x, c)\n",
    "out.shape, skip.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-pendant",
   "metadata": {},
   "source": [
    "### WaveNet全体の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-chaos",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 受容野の大きさを数式通り愚直に計算\n",
    "(2 - 1) * sum([1,2,4,8,16,32,64,128,256,512]) * 3 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-editor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 受容野の大きさを計算する関数\n",
    "from ttslearn.wavenet import receptive_field_size\n",
    "\n",
    "for layers, stacks, kernel_size in [\n",
    "    (30, 3, 2), # WaveNetの論文の設定\n",
    "]:\n",
    "    print(f\"[Layers: {layers}, Dilation cycles: {stacks}, kernel size: {kernel_size}]: recepive field (ミリ秒):\")\n",
    "    size = receptive_field_size(layers, stacks, kernel_size)\n",
    "    print(f\"{size} samples ({size / 16000 * 1000} ミリ秒)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-casino",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WaveNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        out_channels=256,  # 出力のチャネル数\n",
    "        layers=30,  # レイヤー数\n",
    "        stacks=3,  # 畳み込みブロックの数\n",
    "        residual_channels=64,  # 残差結合のチャネル数\n",
    "        gate_channels=128,  # ゲートのチャネル数\n",
    "        skip_out_channels=64,  # スキップ接続のチャネル数\n",
    "        kernel_size=2,  # 1 次元畳み込みのカーネルサイズ\n",
    "        cin_channels=80,  # 条件付け特徴量のチャネル数\n",
    "        upsample_scales=None,  # アップサンプリングのスケール\n",
    "        aux_context_window=0,  # アップサンプリング時に参照する近傍フレーム数\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.cin_channels = cin_channels\n",
    "        self.aux_context_window = aux_context_window\n",
    "        if upsample_scales is None:\n",
    "            upsample_scales = [10, 8]\n",
    "        self.upsample_scales = upsample_scales\n",
    "\n",
    "        self.first_conv = Conv1d1x1(out_channels, residual_channels)\n",
    "\n",
    "        # メインとなる畳み込み層\n",
    "        self.main_conv_layers = nn.ModuleList()\n",
    "        layers_per_stack = layers // stacks\n",
    "        for layer in range(layers):\n",
    "            dilation = 2 ** (layer % layers_per_stack)\n",
    "            conv = ResSkipBlock(\n",
    "                residual_channels,\n",
    "                gate_channels,\n",
    "                kernel_size,\n",
    "                skip_out_channels,\n",
    "                dilation=dilation,\n",
    "                cin_channels=cin_channels,\n",
    "            )\n",
    "            self.main_conv_layers.append(conv)\n",
    "\n",
    "        # スキップ接続の和から波形への変換\n",
    "        self.last_conv_layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.ReLU(),\n",
    "                Conv1d1x1(skip_out_channels, skip_out_channels),\n",
    "                nn.ReLU(),\n",
    "                Conv1d1x1(skip_out_channels, out_channels),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # フレーム単位の特徴量をサンプル単位にアップサンプリング\n",
    "        self.upsample_net = ConvInUpsampleNetwork(\n",
    "            upsample_scales, cin_channels, aux_context_window\n",
    "        )\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        # 量子化された離散値列から One-hot ベクトルに変換\n",
    "        # (B, T) -> (B, T, out_channels) -> (B, out_channels, T)\n",
    "        x = F.one_hot(x, self.out_channels).transpose(1, 2).float()\n",
    "\n",
    "        # 条件付け特徴量のアップサンプリング\n",
    "        c = self.upsample_net(c)\n",
    "\n",
    "        # One-hot ベクトルの次元から隠れ層の次元に変換\n",
    "        x = self.first_conv(x)\n",
    "\n",
    "        # メインの畳み込み層の処理\n",
    "        # 各層におけるスキップ接続の出力を加算して保持\n",
    "        skips = 0\n",
    "        for f in self.main_conv_layers:\n",
    "            x, h = f(x, c)\n",
    "            skips += h\n",
    "\n",
    "        # スキップ接続の和を入力として、出力を計算\n",
    "        x = skips\n",
    "        for f in self.last_conv_layers:\n",
    "            x = f(x)\n",
    "\n",
    "        # NOTE: 出力を確率値として解釈する場合には softmax が必要ですが、\n",
    "        # 学習時には nn.CrossEntropyLoss の計算に置いて softmax の計算が行われるので、\n",
    "        # ここでは明示的に softmax を計算する必要はありません\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-homeless",
   "metadata": {},
   "source": [
    "### トイモデルを利用したWaveNetの動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: inferenceに対応したWaveNetを利用するには、次の行をコメントアウトしてください\n",
    "# from ttslearn.wavenet import WaveNet\n",
    "\n",
    "# ここでは、inference関数の実装を省略します\n",
    "\n",
    "wavenet = WaveNet(out_channels=256, layers=2, stacks=1, kernel_size=2, cin_channels=64)\n",
    "wavenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 から 255 までの値を持つ適当な入力信号\n",
    "x = torch.randint(0, 255, (16, 16000))\n",
    "# フレームシフトを 80 サンプルとして、64 次元の条件付け特徴量を生成\n",
    "c = torch.rand(16, 64, 16000//80)\n",
    "\n",
    "print(\"入力のサイズ:\", tuple(x.shape))\n",
    "print(\"条件付け特徴量のサイズ:\", tuple(c.shape))\n",
    "\n",
    "x_hat = wavenet(x, c)\n",
    "\n",
    "# アップサンプリングの動作確認のために、条件付け特徴量のアップサンプリングのみ実行\n",
    "c_up = wavenet.upsample_net(c)\n",
    "\n",
    "print(\"アップサンプリングされた条件付け特徴量のサイズ:\", tuple(c_up.shape))\n",
    "print(\"WaveNet の出力のサイズ:\", tuple(x_hat.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-librarian",
   "metadata": {},
   "source": [
    "### 負の対数尤度の最小化の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob = F.log_softmax(x_hat, dim=1)\n",
    "# 自己回帰性を保つため、出力を時間方向に1つシフトする\n",
    "nll = nn.NLLLoss()(log_prob[:, :, :-1], x[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss = nn.CrossEntropyLoss()(x_hat[:, :, :-1], x[:, 1:])\n",
    "print(\"nll:\", nll.item())\n",
    "print(\"ce_loss\", ce_loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
